#在激活函数中，对输入数据进行激活操作，是逐元素进行运算的，在运算过程中，没有改变数据的大小，即输入和输出的数据大小是相等的


#Sigmoid,现在一般不用了，因为会发生梯度消失的情况
layer {
  name: "sig"
  type: "Sigmoid"
  bottom: "conv"
  top: "sig"
}

#ReLU,是现在使用最多的激活函数，主要因为其收敛速度快，并且能保持同样的效果。标准的ReLU函数为max(x,0)，当x>0，输出x；当x<=0，输出0
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
